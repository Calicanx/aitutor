import os
import sys
import json
import time
import re
from typing import List, Optional, Dict
from pathlib import Path
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from shared.logging_config import get_logger
from .schema import Memory, MemoryType
from .embeddings import get_embeddings_batch

load_dotenv()

logger = get_logger(__name__)


class MemoryStore:
    def __init__(self, user_id: str = None, index_name: str = None):
        """
        Initialize MemoryStore with user-specific index.
        
        Args:
            user_id: User ID to create/get index named "memory_{user_id}"
            index_name: Optional override for index name (for backward compatibility)
        """
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        
        # Determine index name: user_id-based or provided or env or default
        if user_id:
            # Sanitize user_id for Pinecone index name (must be lowercase alphanumeric with hyphens only)
            sanitized_user_id = self._sanitize_index_name(user_id)
            self.index_name = f"memory-{sanitized_user_id}"
            logger.info(f"Using user-specific index: {self.index_name} (from user_id: {user_id})")
        elif index_name:
            self.index_name = index_name
            logger.info(f"Using provided index: {self.index_name}")
        else:
            # Fallback to env or default (for backward compatibility)
            self.index_name = os.getenv("PINECONE_INDEX_NAME", "aitutor-memories")
            logger.info(f"Using default index: {self.index_name}")
        
        # Check if index exists, create if not
        self._ensure_index_exists()
        
        self.index = self.pc.Index(self.index_name)
    
    def _sanitize_index_name(self, user_id: str) -> str:
        """
        Sanitize user_id to be valid for Pinecone index names.
        Pinecone index names must be lowercase alphanumeric characters or hyphens (-).
        Underscores are NOT allowed, so we replace them with hyphens.
        """
        # Convert to lowercase and replace invalid characters (including underscores) with hyphens
        sanitized = re.sub(r'[^a-z0-9-]', '-', user_id.lower())
        # Replace underscores with hyphens (Pinecone doesn't allow underscores)
        sanitized = sanitized.replace('_', '-')
        # Remove consecutive hyphens
        sanitized = re.sub(r'-+', '-', sanitized)
        # Remove leading/trailing hyphens
        sanitized = sanitized.strip('-')
        # Ensure it's not empty
        if not sanitized:
            sanitized = "anonymous"
        return sanitized
    
    def _ensure_index_exists(self):
        """Check if index exists, create it if it doesn't."""
        try:
            existing_indexes = [idx.name for idx in self.pc.list_indexes()]
            
            if self.index_name not in existing_indexes:
                logger.info(f"Index '{self.index_name}' not found. Creating new index for user...")
                
                # Get embedding dimension from env or default to 1024
                dimension = int(os.getenv("EMBEDDING_DIMENSION", "1024"))
                
                # Get cloud and region from env or use defaults
                cloud = os.getenv("PINECONE_CLOUD", "aws")  # "aws" or "gcp"
                region = os.getenv("PINECONE_REGION", "us-east-1")
                
                self.pc.create_index(
                    name=self.index_name,
                    dimension=dimension,
                    metric="cosine",
                    spec=ServerlessSpec(
                        cloud=cloud,
                        region=region
                    )
                )
                
                # Wait for index to be ready
                logger.info(f"Waiting for index '{self.index_name}' to be ready...")
                max_wait_time = 300  # 5 minutes max wait
                start_time = time.time()
                
                while True:
                    try:
                        index_info = self.pc.describe_index(self.index_name)
                        if index_info.status.get('ready', False):
                            logger.info(f"Index '{self.index_name}' is ready")
                            break
                        
                        elapsed = time.time() - start_time
                        if elapsed > max_wait_time:
                            raise TimeoutError(f"Index '{self.index_name}' did not become ready within {max_wait_time} seconds")
                        
                        time.sleep(2)
                    except Exception as e:
                        elapsed = time.time() - start_time
                        if elapsed > max_wait_time:
                            raise TimeoutError(f"Error waiting for index: {e}")
                        logger.warning(f"Waiting for index... ({e})")
                        time.sleep(2)
            else:
                logger.info(f"Index '{self.index_name}' already exists - using existing index")
                
        except Exception as e:
            logger.error(f"❌ Error checking/creating index: {e}", exc_info=True)
            raise

    def save_memory(self, memory: Memory):
        logger.info(f"Saving single memory: {memory.type.value} - {memory.text[:50]}...")
        try:
            embedding = get_embeddings_batch([memory.text])[0]
            # Filter out None/null values from metadata (Pinecone doesn't accept null values)
            clean_metadata = {k: v for k, v in memory.metadata.items() if v is not None}
            
            self.index.upsert(
                vectors=[{
                    "id": memory.id,
                    "values": embedding,
                    "metadata": {
                        "student_id": memory.student_id,
                        "type": memory.type.value,
                        "text": memory.text,
                        "importance": memory.importance,
                        "timestamp": memory.timestamp.isoformat(),
                        "session_id": memory.session_id,
                        **clean_metadata
                    }
                }],
                namespace=memory.type.value
            )
            logger.info(f"Saved to Pinecone (namespace: {memory.type.value})")
            self._save_to_local(memory)
            logger.info("Saved to local file")
        except Exception as e:
            logger.error(f"❌ Error saving memory: {e}", exc_info=True)
            raise

    def save_memories_batch(self, memories: List[Memory]):
        if not memories:
            logger.warning("[MEMORY_STORE] save_memories_batch called with empty list")
            return

        # Count by type
        type_counts = {}
        for mem in memories:
            mem_type = mem.type.value
            type_counts[mem_type] = type_counts.get(mem_type, 0) + 1

        logger.info("[MEMORY_STORE] Saving batch of %s memories to Pinecone and local storage (breakdown: %s)", 
                   len(memories), type_counts)

        memories_by_type = {}
        for mem in memories:
            if mem.type not in memories_by_type:
                memories_by_type[mem.type] = []
            memories_by_type[mem.type].append(mem)

        for mem_type, mems in memories_by_type.items():
            logger.info("[MEMORY_STORE] Processing %s %s memories...", len(mems), mem_type.value)
            
            # Log each memory being saved
            for i, mem in enumerate(mems, 1):
                emotion_str = f", emotion: {mem.metadata.get('emotion', 'none')}" if mem.metadata.get('emotion') else ""
                logger.info("[MEMORY_STORE]   Memory %s/%s: [%s] (importance: %.2f%s) - %s",
                          i, len(mems), mem_type.value.upper(), mem.importance, emotion_str, mem.text[:80])
            
            texts = [m.text for m in mems]
            
            try:
                embeddings = get_embeddings_batch(texts)
                logger.info("[MEMORY_STORE] Generated embeddings for %s %s memories", len(embeddings), mem_type.value)

                vectors = []
                for mem, emb in zip(mems, embeddings):
                    # Filter out None/null values from metadata (Pinecone doesn't accept null values)
                    clean_metadata = {k: v for k, v in mem.metadata.items() if v is not None}
                    
                    vectors.append({
                        "id": mem.id,
                        "values": emb,
                        "metadata": {
                            "student_id": mem.student_id,
                            "type": mem.type.value,
                            "text": mem.text,
                            "importance": mem.importance,
                            "timestamp": mem.timestamp.isoformat(),
                            "session_id": mem.session_id,
                            **clean_metadata
                        }
                    })

                self.index.upsert(vectors=vectors, namespace=mem_type.value)
                logger.info("[MEMORY_STORE] Saved %s vectors to Pinecone (namespace: %s)", len(vectors), mem_type.value)
            except Exception as e:
                logger.error("[MEMORY_STORE] Error saving %s memories to Pinecone: %s", mem_type.value, e, exc_info=True)
                raise

        # Save to local files
        logger.info("[MEMORY_STORE] Saving %s memories to local JSON files...", len(memories))
        saved_count = 0
        for mem in memories:
            try:
                self._save_to_local(mem)
                saved_count += 1
            except Exception as e:
                logger.error("[MEMORY_STORE] Error saving memory %s to local file: %s", mem.id, e, exc_info=True)
        
        logger.info("[MEMORY_STORE] Successfully saved all %s memories (%s to Pinecone, %s to local files)", 
                   len(memories), len(memories), saved_count)

    def search(self, query: str, student_id: str, mem_type: Optional[MemoryType] = None, 
               top_k: int = 10, exclude_session_id: Optional[str] = None) -> List[Dict]:
        from .embeddings import get_query_embedding
        
        # Sanitize query snippet for console encodings that may not support all Unicode
        snippet = (query or "")[:50]
        safe_snippet = snippet.encode("ascii", "ignore").decode("ascii", "ignore")
        logger.info(
            "[MEMORY_STORE] Searching in index: %s for student_id: %s, query: %s...",
            self.index_name,
            student_id,
            safe_snippet,
        )
        
        query_embedding = get_query_embedding(query)
        filter_dict = {"student_id": {"$eq": student_id}}
        
        if exclude_session_id:
            filter_dict["session_id"] = {"$ne": exclude_session_id}

        namespaces = [mem_type.value] if mem_type else [mt.value for mt in MemoryType]
        logger.info("[MEMORY_STORE]   Searching namespaces: %s, top_k: %s, filter: %s", namespaces, top_k, filter_dict)

        results = []
        for namespace in namespaces:
            try:
                response = self.index.query(
                    vector=query_embedding,
                    top_k=top_k,
                    namespace=namespace,
                    filter=filter_dict,
                    include_metadata=True
                )
                logger.info("[MEMORY_STORE]   Namespace '%s': Found %s matches", namespace, len(response.matches))
                for i, match in enumerate(response.matches):
                    # Skip matches with missing metadata
                    if not match.metadata:
                        logger.warning(f"   Match {i} in namespace '{namespace}' has no metadata, skipping")
                        continue

                    try:
                        # Reconstruct metadata structure for Memory.from_dict()
                        # Pinecone stores flattened metadata (emotion, valence, etc. at top level)
                        # But Memory.from_dict() expects nested structure with 'metadata' dict
                        metadata_dict = match.metadata.copy()
                        
                        # Extract nested metadata fields (emotion, valence, category, topic, etc.)
                        # These are stored at top level in Pinecone but should be in nested 'metadata' dict
                        nested_metadata = {}
                        memory_fields = {'id', 'type', 'text', 'importance', 'student_id', 'session_id', 'timestamp'}
                        
                        for key, value in list(metadata_dict.items()):
                            if key not in memory_fields:
                                nested_metadata[key] = value
                                metadata_dict.pop(key)
                        
                        # Add nested metadata dict
                        metadata_dict['metadata'] = nested_metadata
                        
                        memory = Memory.from_dict(metadata_dict)
                        results.append({
                            "memory": memory,
                            "score": match.score
                        })
                        logger.debug(f"   ✅ Converted match {i}: {memory.text[:50]}... (score: {match.score:.3f})")
                    except Exception as e:
                        logger.error(f"   ❌ Error converting match {i} in namespace '{namespace}': {e}", exc_info=True)
                        logger.error(f"   Metadata keys: {list(match.metadata.keys())}")
                        continue
            except Exception as e:
                logger.error(f"❌ Error searching namespace '{namespace}' in index '{self.index_name}': {e}", exc_info=True)

        results.sort(key=lambda x: x["score"], reverse=True)
        final_results = results[:top_k]
        logger.info(
            "[MEMORY_STORE] Search complete: Returning %s results from index '%s'",
            len(final_results),
            self.index_name,
        )
        return final_results

    def _save_to_local(self, memory: Memory):
        data_dir = f"services/TeachingAssistant/Memory/data/{memory.student_id}/memory"
        os.makedirs(data_dir, exist_ok=True)
        
        file_path = f"{data_dir}/{memory.type.value}.json"
        memories = []
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                memories = json.load(f)
        
        # Check for duplicate memory ID before appending
        memory_dict = memory.to_dict()
        existing_ids = {m.get('id') for m in memories if isinstance(m, dict)}
        
        if memory_dict['id'] not in existing_ids:
            memories.append(memory_dict)
        else:
            # Update existing memory instead of duplicating
            for i, m in enumerate(memories):
                if isinstance(m, dict) and m.get('id') == memory_dict['id']:
                    memories[i] = memory_dict
                    break
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(memories, f, indent=2, ensure_ascii=False)




